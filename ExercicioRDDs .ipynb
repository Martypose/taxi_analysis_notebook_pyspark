{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cargar datos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('datasets/yellow_tripdata_2018-11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VendorID\n",
    "* tpep_pickup_datetime\n",
    "* tpep_dropoff_datetime\n",
    "* passenger_count\n",
    "* trip_distance\n",
    "* RatecodeID\n",
    "* store_and_fwd_flag\n",
    "* PULocationID\n",
    "* DOLocationID\n",
    "* payment_type\n",
    "* fare_amount\n",
    "* extra\n",
    "* mta_tax\n",
    "* tip_amount\n",
    "* tolls_amount\n",
    "* improvement_surcharge\n",
    "* total_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount', u'', u'1,2018-11-01 00:51:36,2018-11-01 00:52:36,1,.00,1,N,145,145,2,2.5,0.5,0.5,0,0,0.3,3.8', u'1,2018-11-01 00:07:47,2018-11-01 00:21:43,1,2.30,1,N,142,164,1,11,0.5,0.5,2.45,0,0.3,14.75', u'1,2018-11-01 00:24:27,2018-11-01 00:34:29,1,1.80,1,N,164,48,2,8.5,0.5,0.5,0,0,0.3,9.8']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_cabecera_fila_vacia(line):\n",
    "    if line.startswith(\"VendorID\") or line.strip() == \"\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Filtra el RDD original 'rdd' con la función 'eliminar_cabecera_fila_vacia'\n",
    "rdd_filtrado = rdd.filter(eliminar_cabecera_fila_vacia)\n",
    "\n",
    "# Luego, utiliza 'map' para dividir cada línea en campos\n",
    "datosSeparados = rdd_filtrado.map(lambda line: line.split(','))\n",
    "    \n",
    "\n",
    "# Ahora 'limpiarCabeceraFilaVacia' contendrá el RDD sin las dos primeras líneas\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1',\n",
       "  u'2018-11-01 00:51:36',\n",
       "  u'2018-11-01 00:52:36',\n",
       "  u'1',\n",
       "  u'.00',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'145',\n",
       "  u'145',\n",
       "  u'2',\n",
       "  u'2.5',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'3.8'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:07:47',\n",
       "  u'2018-11-01 00:21:43',\n",
       "  u'1',\n",
       "  u'2.30',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'142',\n",
       "  u'164',\n",
       "  u'1',\n",
       "  u'11',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'2.45',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'14.75'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:24:27',\n",
       "  u'2018-11-01 00:34:29',\n",
       "  u'1',\n",
       "  u'1.80',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'164',\n",
       "  u'48',\n",
       "  u'2',\n",
       "  u'8.5',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'9.8'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:35:27',\n",
       "  u'2018-11-01 00:47:02',\n",
       "  u'1',\n",
       "  u'2.30',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'48',\n",
       "  u'107',\n",
       "  u'1',\n",
       "  u'10',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'3.35',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'14.65'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:16:46',\n",
       "  u'2018-11-01 00:22:50',\n",
       "  u'1',\n",
       "  u'1.00',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'163',\n",
       "  u'170',\n",
       "  u'2',\n",
       "  u'6',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'7.3'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:23:57',\n",
       "  u'2018-11-01 00:34:29',\n",
       "  u'1',\n",
       "  u'2.10',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'170',\n",
       "  u'79',\n",
       "  u'2',\n",
       "  u'9',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'10.3'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:57:07',\n",
       "  u'2018-11-01 01:05:27',\n",
       "  u'1',\n",
       "  u'1.60',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'148',\n",
       "  u'79',\n",
       "  u'2',\n",
       "  u'7.5',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'8.8'],\n",
       " [u'2',\n",
       "  u'2018-11-01 01:03:28',\n",
       "  u'2018-11-01 01:32:10',\n",
       "  u'2',\n",
       "  u'22.09',\n",
       "  u'2',\n",
       "  u'N',\n",
       "  u'132',\n",
       "  u'162',\n",
       "  u'2',\n",
       "  u'52',\n",
       "  u'0',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'5.76',\n",
       "  u'0.3',\n",
       "  u'58.56'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:05:28',\n",
       "  u'2018-11-01 00:14:50',\n",
       "  u'1',\n",
       "  u'1.60',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'158',\n",
       "  u'100',\n",
       "  u'2',\n",
       "  u'8',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'9.3'],\n",
       " [u'1',\n",
       "  u'2018-11-01 00:30:17',\n",
       "  u'2018-11-01 00:41:39',\n",
       "  u'2',\n",
       "  u'1.80',\n",
       "  u'1',\n",
       "  u'N',\n",
       "  u'90',\n",
       "  u'246',\n",
       "  u'2',\n",
       "  u'9.5',\n",
       "  u'0.5',\n",
       "  u'0.5',\n",
       "  u'0',\n",
       "  u'0',\n",
       "  u'0.3',\n",
       "  u'10.8']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datosSeparados.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A) Contar o número de viaxes entre 00:00 e 01:00 de cada día e dar o total por día (agrupados por día, tódolos días teñen un reconto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def es_entre_00_01(pickup_str, dropoff_str):\n",
    "    pickup_hora = datetime.strptime(pickup_str, '%Y-%m-%d %H:%M:%S').time()\n",
    "    dropoff_hora = datetime.strptime(dropoff_str, '%Y-%m-%d %H:%M:%S').time()\n",
    "    return pickup_hora.hour == 0 and dropoff_hora.hour == 0\n",
    "\n",
    "# Filtrar las filas con tiempo de recogida y entrega entre las 00:00 y las 00:59\n",
    "viajes_00_01 = datosSeparados.filter(lambda row: es_entre_00_01(row[1], row[2]))\n",
    "\n",
    "\n",
    "# Extraer la fecha (día) de la columna 'tpep_pickup_datetime'\n",
    "viajes_00_01_fecha = viajes_00_01.map(lambda row: (datetime.strptime(row[1], '%Y-%m-%d %H:%M:%S').date(), 1))\n",
    "\n",
    "# Agrupar por la fecha y contar el número de viajes en cada grupo\n",
    "conteo_por_dia = viajes_00_01_fecha.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 'conteo_por_dia' es un RDD con el conteo de viajes entre 00:00 y 01:00 de cada día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2018, 11, 27), 3980),\n",
       " (datetime.date(2018, 11, 15), 5948),\n",
       " (datetime.date(2018, 11, 17), 11329),\n",
       " (datetime.date(2018, 11, 5), 3124),\n",
       " (datetime.date(2018, 11, 9), 8294),\n",
       " (datetime.date(2018, 11, 23), 3257),\n",
       " (datetime.date(2018, 11, 20), 4797),\n",
       " (datetime.date(2018, 11, 24), 6311),\n",
       " (datetime.date(2018, 11, 12), 3660),\n",
       " (datetime.date(2018, 12, 1), 83),\n",
       " (datetime.date(2009, 1, 1), 6),\n",
       " (datetime.date(2018, 11, 30), 7971),\n",
       " (datetime.date(2018, 11, 2), 7181),\n",
       " (datetime.date(2018, 11, 25), 7697),\n",
       " (datetime.date(2018, 11, 21), 5751),\n",
       " (datetime.date(2018, 11, 3), 10516),\n",
       " (datetime.date(2018, 11, 13), 4307),\n",
       " (datetime.date(2018, 11, 28), 5416),\n",
       " (datetime.date(2018, 11, 6), 3810),\n",
       " (datetime.date(2018, 11, 18), 12039),\n",
       " (datetime.date(2018, 11, 10), 10748),\n",
       " (datetime.date(2018, 11, 1), 7682),\n",
       " (datetime.date(2018, 11, 11), 11939),\n",
       " (datetime.date(2018, 11, 29), 6011),\n",
       " (datetime.date(2018, 11, 19), 2960),\n",
       " (datetime.date(2018, 11, 7), 4974),\n",
       " (datetime.date(2018, 11, 14), 5212),\n",
       " (datetime.date(2018, 11, 16), 6613),\n",
       " (datetime.date(2018, 11, 4), 11679),\n",
       " (datetime.date(2018, 11, 8), 5821),\n",
       " (datetime.date(2018, 11, 22), 5842),\n",
       " (datetime.date(2018, 11, 26), 3388)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conteo_por_dia.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* B) Contar o número de viaxes entre 00:00 e 01:00 de cada día e dar o total por mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer el año y mes de cada fecha y contar los viajes por mes\n",
    "conteo_por_mes = conteo_por_dia.map(lambda row: ((row[0].year, row[0].month), row[1])).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2018, 12), 83), ((2009, 1), 6), ((2018, 11), 198257)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conteo_por_mes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C) A media de viaxes ao mes que fai cada conductor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Voy a leer de nuevo el archivo para obtener el dataframe de spark del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "viajes = spark.read.csv('datasets/yellow_tripdata_2018-11.csv', header=True,\n",
    "                       inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viajes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "viajes.createOrReplaceTempView('viajes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|VendorID|mes|num_viajes|\n",
      "+--------+---+----------+\n",
      "|       4| 11|    130817|\n",
      "|       1| 11|   3262928|\n",
      "|       2|  1|        26|\n",
      "|       2| 11|   4751001|\n",
      "|       2| 12|       123|\n",
      "|       2| 10|       269|\n",
      "+--------+---+----------+\n",
      "\n",
      "+--------+-----------------------+\n",
      "|VendorID|promedio_viajes_por_mes|\n",
      "+--------+-----------------------+\n",
      "|       1|              3262928.0|\n",
      "|       4|               130817.0|\n",
      "|       2|             1187854.75|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta la consulta SQL para obtener el conteo de viajes por VendorID y mes\n",
    "countDF = spark.sql('SELECT VendorID, Month(tpep_pickup_datetime) as mes, count(*) as num_viajes FROM viajes group by VendorID, Month(tpep_pickup_datetime)')\n",
    "\n",
    "# Muestra el resultado de la consulta anterior\n",
    "countDF.show()\n",
    "\n",
    "# Crea una vista temporal para ejecutar consultas SQL en countDF\n",
    "countDF.createOrReplaceTempView('conteo_viajes')\n",
    "\n",
    "# Ejecuta una consulta SQL para calcular el promedio de viajes por mes para cada VendorID\n",
    "viajes_promedio = spark.sql('SELECT VendorID, AVG(num_viajes) as promedio_viajes_por_mes FROM conteo_viajes GROUP BY VendorID')\n",
    "\n",
    "# Muestra el resultado de la consulta anterior\n",
    "viajes_promedio.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* D) A media de viaxes ao día que fai cada conductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|VendorID|promedio_viajes_por_dia|\n",
      "+--------+-----------------------+\n",
      "|       1|     108764.26666666666|\n",
      "|       4|      4360.566666666667|\n",
      "|       2|             118785.475|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular la media de viajes al día que hace cada conductor (VendorID)\n",
    "media_viajes_por_dia = spark.sql(\"\"\"\n",
    "    SELECT VendorID, \n",
    "           AVG(num_viajes) as promedio_viajes_por_dia\n",
    "    FROM (\n",
    "        SELECT VendorID, \n",
    "               DATE(tpep_pickup_datetime) as fecha, \n",
    "               COUNT(*) as num_viajes\n",
    "        FROM viajes\n",
    "        GROUP BY VendorID, DATE(tpep_pickup_datetime)\n",
    "    ) as viajes_por_dia\n",
    "    GROUP BY VendorID\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "media_viajes_por_dia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E) Cantos pasaxeiros foron como máximo na primeira semana do mes (nunha viaxe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|max_pasajeros_primera_semana|\n",
      "+----------------------------+\n",
      "|                           9|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar viajes en la primera semana del mes y seleccionar el máximo número de pasajeros en una consulta SQL\n",
    "max_pasajeros_primera_semana = spark.sql(\"\"\"\n",
    "    SELECT MAX(passenger_count) as max_pasajeros_primera_semana\n",
    "    FROM viajes\n",
    "    WHERE DAY(tpep_pickup_datetime) BETWEEN 1 AND 7\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "max_pasajeros_primera_semana.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* F) Cantos pasaxeiros foron como máximo en todo o mes (nunha viaxe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|mes|max_pasajeros|\n",
      "+---+-------------+\n",
      "| 12|            6|\n",
      "|  1|            5|\n",
      "| 10|            6|\n",
      "| 11|           96|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_pasajeros_por_mes = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Month(tpep_pickup_datetime) as mes,\n",
    "        MAX(passenger_count) as max_pasajeros\n",
    "    FROM viajes\n",
    "    GROUP BY Month(tpep_pickup_datetime)\n",
    "\"\"\")\n",
    "max_pasajeros_por_mes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* G) Cantos cartos costou o percorrido máis caro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|costo_mas_caro|\n",
      "+--------------+\n",
      "|     187437.76|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_costo = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MAX(total_amount) as costo_mas_caro\n",
    "    FROM viajes\n",
    "\"\"\")\n",
    "max_costo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H) Cantos cartos costou o percorrido máis barato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|costo_mas_barato|\n",
      "+----------------+\n",
      "|          -450.3|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_costo = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MIN(total_amount) as costo_mas_barato\n",
    "    FROM viajes\n",
    "\"\"\")\n",
    "min_costo.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
